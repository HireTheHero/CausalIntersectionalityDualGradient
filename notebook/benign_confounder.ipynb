{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# benign confounders labeling for hateful memes"
      ],
      "metadata": {
        "id": "CgHePnxxf71j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## init"
      ],
      "metadata": {
        "id": "J_LoTco3gA2h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USd975QgMl7s"
      },
      "outputs": [],
      "source": [
        "# mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# hateful memes from MyDrive\n",
        "!mkdir data\n",
        "!unzip /content/drive/MyDrive/vilio/hateful_memes.zip -d data"
      ],
      "metadata": {
        "id": "gW32xaw5hXqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Noixas/Multimodal-NLP\n",
        "!mkdir data/confounder\n",
        "!cp Multimodal-NLP/dataset/upsampling_img_hash/image_confounders_id.json data/confounder\n",
        "!cp Multimodal-NLP/dataset/upsampling_img_hash/image_confounders_id_dev_data.json data/confounder"
      ],
      "metadata": {
        "id": "mMICt9L9Pf7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modules\n",
        "import glob\n",
        "from itertools import chain\n",
        "import json\n",
        "import os\n",
        "import signal\n",
        "import sys\n",
        "import time\n",
        "from typing import Dict, List, Union\n",
        "import warnings\n",
        "\n",
        "import cv2\n",
        "import dask.dataframe as dd\n",
        "from dask.delayed import delayed\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "# from matplotlib import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import plotly.graph_objects as go\n",
        "# from scipy.spatial import distance\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('dark_background')\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "IhCAlx9xuIP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# variables\n",
        "data_root = \"/content/data/hateful_memes\"\n",
        "img_path = \"img\"\n",
        "jsons = []\n",
        "txt_col, img_col, label_col = \"text\", \"img\", \"label\"\n",
        "txt_cf_col, img_cf_col = \"is_txt_confounder\", \"is_img_confounder\"\n",
        "txt_id_col, img_id_col = \"txt_org_id\", \"img_org_id\"\n",
        "export_path = \"/content/data/confounder\""
      ],
      "metadata": {
        "id": "sRJot8mEt95Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocessing"
      ],
      "metadata": {
        "id": "_XWDFR1SvZ4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### jsonl"
      ],
      "metadata": {
        "id": "oahen-xIvb_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_jsonls(path: str):\n",
        "    \"\"\"\n",
        "    Read all jsonl files\n",
        "    \"\"\"\n",
        "\n",
        "    def _read_jsonl(filename: str, usecols:List[str]=['id', 'img', label_col, 'text', \"split\"]):\n",
        "        \"\"\"\n",
        "        Read one jsonl file w/ idenfitier\n",
        "        \"\"\"\n",
        "        df = pd.read_json(filename, orient='records', lines=True)\n",
        "        df[\"split\"] = filename.split(\"/\")[-1].split(\".jsonl\")[0]\n",
        "        if label_col not in df.columns:\n",
        "            df[label_col] = -1\n",
        "        return df[usecols]\n",
        "\n",
        "    jsons = glob.glob(f\"{path}/*.jsonl\")\n",
        "    dd_list = [delayed(_read_jsonl)(json) for json in jsons]\n",
        "    df = dd.from_delayed(dd_list).compute()\n",
        "    return df"
      ],
      "metadata": {
        "id": "URTosmwUvdvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = read_jsonls(data_root)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "y9qzg63QxObv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for compatibility with image filename\n",
        "df[\"id\"] = df[\"id\"].astype('str').str.zfill(5)"
      ],
      "metadata": {
        "id": "H19mY46hayQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## detecting benign confounder"
      ],
      "metadata": {
        "id": "7lCmadYXhrC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### same-text-diff-image"
      ],
      "metadata": {
        "id": "s4FOGLWMh1ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# flag duplicates\n",
        "df_cnt = df[txt_col].value_counts().reset_index()\n",
        "df_cnt.columns = [txt_col, \"cnt\"]\n",
        "txt_dup = df_cnt.loc[df_cnt[\"cnt\"]>=2, txt_col].tolist()\n",
        "df[txt_cf_col] = 0\n",
        "df.loc[df[txt_col].isin(txt_dup), txt_cf_col] = 1\n",
        "df[txt_cf_col].value_counts()"
      ],
      "metadata": {
        "id": "bPwPhgvAhox3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### same-image-diff-text"
      ],
      "metadata": {
        "id": "S3_n9HlWh4Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load duplicates\n",
        "with open(os.path.join(export_path, \"image_confounders_id.json\"), \"r\") as f:\n",
        "    img_dup = json.load(f)\n",
        "with open(os.path.join(export_path, \"image_confounders_id_dev_data.json\"), \"r\") as f:\n",
        "    img_dup_dev = json.load(f)"
      ],
      "metadata": {
        "id": "-_IB3iLTTAnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make it compatible with filename\n",
        "img_dup = [str(img).zfill(5) for img in img_dup]\n",
        "img_dup_dev = [str(img).zfill(5) for img in img_dup_dev]\n",
        "img_dup_all = img_dup+img_dup_dev"
      ],
      "metadata": {
        "id": "mWz4LxStTnL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check duplicates\n",
        "_idx, _idx2 = 1, 2\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "#subplot(r,c) provide the no. of rows and columns\n",
        "f, axarr = plt.subplots(1,2,figsize=(12,16))\n",
        "for i,idx in enumerate([_idx,_idx2]):\n",
        "    # use the created array to output your multiple images.\n",
        "    img_id = img_dup[idx]\n",
        "    img_path = f\"img/{img_id}.png\"\n",
        "    label = df.loc[df[\"id\"]==img_id, label_col].values[0]\n",
        "    img = cv2.imread(os.path.join(data_root, img_path), cv2.IMREAD_GRAYSCALE)\n",
        "    axarr[i].set_title(f\"id={img_id}, label={label}\")\n",
        "    axarr[i].imshow(img, cmap=\"gray\")"
      ],
      "metadata": {
        "id": "zpzMVtBOh48X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flag duplicates\n",
        "df[img_cf_col] = 0\n",
        "df.loc[df[\"id\"].isin(img_dup_all), img_cf_col] = 1\n",
        "df[img_cf_col].value_counts()"
      ],
      "metadata": {
        "id": "a1tkceN4SIOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### explore confounder"
      ],
      "metadata": {
        "id": "YsqZ1VGkUK-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text*image confounder cross aggregates\n",
        "df.groupby([txt_cf_col, img_cf_col])[\"id\"].nunique()"
      ],
      "metadata": {
        "id": "wzkiexQrUS4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "0-1, 1-0 should be image / text confounder, respectively\n",
        "1-1 should be original (hateful?) image\n",
        "'''"
      ],
      "metadata": {
        "id": "6QGNbOZQUnhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text*image confounder cross aggregates\n",
        "df.groupby([label_col, txt_cf_col, img_cf_col])[\"id\"].nunique()"
      ],
      "metadata": {
        "id": "dVKwLDBAKKSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "text hateful: 1-1-[0,1]\n",
        "text benign: 1-0-[0,1]\n",
        "image hateful: 1-[0,1]-1\n",
        "image benign: 0-[0,1]-1\n",
        "-> once group of confounders is identified, can compare within group\n",
        "'''"
      ],
      "metadata": {
        "id": "VKLFZgOjKnwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## grouping benign confounder"
      ],
      "metadata": {
        "id": "LHlH2F-aN3vW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### text"
      ],
      "metadata": {
        "id": "i7wKbfRgN-XM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select perfect duplicate\n",
        "def duplicates_flagger(data:pd.DataFrame, data2:pd.DataFrame, mod_col:str=txt_col, label_col:str=label_col):\n",
        "    def duplicate_flagger(ip1:Dict[str, Union[str, int]], ip2:Dict[str, Union[str, int]], mode:str=\"perfect\"):\n",
        "        \"\"\"\n",
        "        Compare txt/txt or img/img to check duplicate\n",
        "        \"\"\"\n",
        "        if mode==\"perfect\":\n",
        "            if ip1[\"data\"]==ip2[\"data\"]:\n",
        "                out = ip2[\"id\"]\n",
        "            else:\n",
        "                out = \"Not hit\"\n",
        "        return out\n",
        "    \"\"\"\n",
        "    Flagging duplicates out of single modality\n",
        "    \"\"\"\n",
        "    if mod_col==\"text\":\n",
        "        data_ip = data.copy()\n",
        "        data_ip2 = data2[data2[label_col]==1].drop_duplicates(subset=mod_col)\n",
        "    else:\n",
        "        data_ip = data.copy()\n",
        "    dup_ids = []\n",
        "    for i, ip in tqdm(zip(data_ip[\"id\"], data_ip[mod_col])):\n",
        "        data_dict = {\"id\": i, \"data\": ip}\n",
        "        cnt = 0\n",
        "        for i2, ip2 in zip(data_ip2[\"id\"], data_ip2[mod_col]):\n",
        "            data_dict2 = {\"id\": i2, \"data\": ip2}\n",
        "            dup_id = duplicate_flagger(data_dict, data_dict2)\n",
        "            if dup_id!=\"Not hit\":\n",
        "                dup_ids.append(dup_id)\n",
        "                break\n",
        "            else:\n",
        "                cnt += 1\n",
        "                if cnt==len(data_ip2):\n",
        "                    dup_ids.append(dup_id)\n",
        "                else:\n",
        "                    pass\n",
        "    return dup_ids"
      ],
      "metadata": {
        "id": "XJfD16VTOCJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_txt_cf = df[df[txt_cf_col]==1].copy()\n",
        "df_txt_cf = df[df[txt_cf_col]==1].sort_values(by=txt_col)\n",
        "df_txt_cf[txt_id_col] = duplicates_flagger(df_txt_cf, df_txt_cf)"
      ],
      "metadata": {
        "id": "mNCM9Vkoweza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_txt_cf[txt_id_col].value_counts().head()"
      ],
      "metadata": {
        "id": "9Nk-xL9YZA0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "df_plot = df_txt_cf[df_txt_cf[txt_id_col]!=\"Not hit\"]\n",
        "df_cnt = df_plot.loc[df_plot[txt_id_col]!=\"Not hit\", txt_id_col].value_counts().reset_index()\n",
        "fig.add_trace(go.Histogram(x=df_cnt[txt_id_col], name=f\"id count\", histnorm='probability', bingroup=1))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "oHXfR4OTzlaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### image"
      ],
      "metadata": {
        "id": "ZdqFPS5-OACy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### annotation"
      ],
      "metadata": {
        "id": "bPlN09JJ5elz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# img_dup_all = img_dup_all[:20]\n",
        "# print(len(img_dup_all))"
      ],
      "metadata": {
        "id": "XXbKx6rXq_sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def signal_handler(signal, frame):\n",
        "    print('Aborted')\n",
        "    sys.exit(0)"
      ],
      "metadata": {
        "id": "2AU6K4N7P71f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signal.signal(signal.SIGINT, signal_handler)\n",
        "\n",
        "num_img_org = len(img_dup_all)\n",
        "cluster = 0\n",
        "try:\n",
        "    with open(\"/content/drive/MyDrive/annotation/hm_img_confounder.pickle\", \"rb\") as f:\n",
        "        clusters = pickle.load(f)\n",
        "except Exception as e:\n",
        "    clusters = []\n",
        "if len(clusters)==0:\n",
        "    cluster = 0\n",
        "else:\n",
        "    cluster = max(clusters)\n",
        "done = len(clusters)\n",
        "num_img = num_img_org-done\n",
        "for idx in trange(num_img//2):\n",
        "# for idx in trange(5):\n",
        "    # check duplicates\n",
        "    _idx, _idx2 = done+2*idx, done+2*idx+1\n",
        "\n",
        "    if done:\n",
        "        img_id = img_dup_all[idx]\n",
        "        img_path = f\"img/{img_id}.png\"\n",
        "        label = df.loc[df[\"id\"]==img_id, label_col].values[0]\n",
        "        img = cv2.imread(os.path.join(data_root, img_path), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    # show images\n",
        "    ## subplot(r,c) provide the no. of rows and columns\n",
        "    f, axarr = plt.subplots(1,2,figsize=(12,16))\n",
        "    for i,i2 in enumerate([_idx,_idx2]):\n",
        "        ## use the created array to output your multiple images.\n",
        "        img_id = img_dup_all[i2]\n",
        "        img_path = f\"img/{img_id}.png\"\n",
        "        label = df.loc[df[\"id\"]==img_id, label_col].values[0]\n",
        "        img = cv2.imread(os.path.join(data_root, img_path), cv2.IMREAD_GRAYSCALE)\n",
        "        axarr[i].set_title(f\"id={img_id}, label={label}\")\n",
        "        axarr[i].imshow(img, cmap=\"gray\")\n",
        "    plt.show()\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/annotation/hm_img_confounder.pickle\", \"wb\") as f:\n",
        "        pickle.dump(clusters, f)\n",
        "\n",
        "    ## 1 if image is new, else 0\n",
        "    left = input(\"LEFT image new?: \")\n",
        "    right = input(\"RIGHT image new?: \")\n",
        "    for result in [left, right]:\n",
        "        if int(result):\n",
        "            cluster += 1\n",
        "        else:\n",
        "            pass\n",
        "        clusters.append(cluster)\n",
        "\n",
        "    time.sleep(1)\n",
        "    clear_output(wait=True)\n",
        "with open(\"/content/drive/MyDrive/annotation/hm_img_confounder.pickle\", \"wb\") as f:\n",
        "    pickle.dump(clusters, f)"
      ],
      "metadata": {
        "id": "E4LXjGwKDqzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # check result\n",
        "# arr_clusters = np.array(clusters)\n",
        "# arr_img = np.array(img_dup_all)[:len(arr_clusters)]\n",
        "# for idx in sorted(list(set(clusters)))[1000:1500]:\n",
        "#     print(idx)\n",
        "#     img_cluster = arr_img[arr_clusters==idx]\n",
        "#     if len(img_cluster)==1:\n",
        "#         continue\n",
        "#     plt.figure()\n",
        "#     f, axarr = plt.subplots(1,len(img_cluster),figsize=(4*len(img_cluster),6*len(img_cluster)))\n",
        "#     for i,img_id in enumerate(img_cluster):\n",
        "#         img_path = f\"img/{img_id}.png\"\n",
        "#         label = df.loc[df[\"id\"]==img_id, label_col].values[0]\n",
        "#         img = cv2.imread(os.path.join(data_root, img_path), cv2.IMREAD_GRAYSCALE)\n",
        "#         axarr[i].set_title(f\"id={img_id}, label={label}, cluster={idx}\")\n",
        "#         axarr[i].imshow(img, cmap=\"gray\")\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "2Fkk98SVnHRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # check result\n",
        "# ## single-image cluster + representative image\n",
        "# mode = \"single\"\n",
        "# arr_clusters = np.array(clusters)\n",
        "# arr_img = np.array(img_dup_all)[:len(arr_clusters)]\n",
        "# for idx in sorted(list(set(clusters))):\n",
        "#     img_cluster = arr_img[arr_clusters==idx]\n",
        "#     if (mode==\"single\" and len(img_cluster)==1) or (mode!=\"single\" and len(img_cluster)>1):\n",
        "#         print(idx)\n",
        "#         img_id = img_cluster[0]\n",
        "#         img_path = f\"img/{img_id}.png\"\n",
        "#         label = df.loc[df[\"id\"]==img_id, label_col].values[0]\n",
        "#         img = cv2.imread(os.path.join(data_root, img_path), cv2.IMREAD_GRAYSCALE)\n",
        "#         plt.title(f\"id={img_id}, label={label}, cluster={idx}, num_img={len(img_cluster)}\")\n",
        "#         plt.imshow(img, cmap=\"gray\")\n",
        "#         plt.show()\n",
        "#     else:\n",
        "#         pass"
      ],
      "metadata": {
        "id": "hb0C1JNIDuss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collect\n",
        "arr_clusters,arr_clusters2 = np.array(clusters),np.array(clusters)\n",
        "arr_clusters2[arr_clusters==98] = 106\n",
        "arr_clusters2[arr_clusters==101] = 110\n",
        "arr_clusters2[arr_clusters==495] = 496\n",
        "clusters = arr_clusters2.tolist()\n",
        "with open(\"/content/drive/MyDrive/annotation/hm_img_confounder.pickle\", \"wb\") as f:\n",
        "    pickle.dump(clusters, f)"
      ],
      "metadata": {
        "id": "aa2rEP1xIDpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # check result\n",
        "# ## single-image cluster + representative image\n",
        "# mode = \"multiple\"\n",
        "# arr_clusters = np.array(clusters)\n",
        "# arr_img = np.array(img_dup_all)[:len(arr_clusters)]\n",
        "# for idx in sorted(list(set(clusters))):\n",
        "#     img_cluster = arr_img[arr_clusters==idx]\n",
        "#     if (mode==\"single\" and len(img_cluster)==1) or (mode!=\"single\" and len(img_cluster)>1):\n",
        "#         print(idx)\n",
        "#         img_id = img_cluster[0]\n",
        "#         img_path = f\"img/{img_id}.png\"\n",
        "#         label = df.loc[df[\"id\"]==img_id, label_col].values[0]\n",
        "#         img = cv2.imread(os.path.join(data_root, img_path), cv2.IMREAD_GRAYSCALE)\n",
        "#         plt.title(f\"id={img_id}, label={label}, cluster={idx}, num_img={len(img_cluster)}\")\n",
        "#         plt.imshow(img, cmap=\"gray\")\n",
        "#         plt.show()\n",
        "#     else:\n",
        "#         pass"
      ],
      "metadata": {
        "id": "XVr7JZNNGfel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collect\n",
        "arr_clusters,arr_clusters2 = np.array(clusters),np.array(clusters)\n",
        "arr_clusters2[arr_clusters==620] = 617\n",
        "clusters = arr_clusters2.tolist()\n",
        "with open(\"/content/drive/MyDrive/annotation/hm_img_confounder.pickle\", \"wb\") as f:\n",
        "    pickle.dump(clusters, f)"
      ],
      "metadata": {
        "id": "UBF9ua1xK5wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add cluster for last image\n",
        "## check if it's identical to -2\n",
        "img_id = img_dup_all[-1]\n",
        "img_path = f\"img/{img_id}.png\"\n",
        "label = df.loc[df[\"id\"]==img_id, label_col].values[0]\n",
        "img = cv2.imread(os.path.join(data_root, img_path), cv2.IMREAD_GRAYSCALE)\n",
        "plt.imshow(img, cmap=\"gray\")"
      ],
      "metadata": {
        "id": "ZT3dEXGwO51v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add cluster for last image\n",
        "clusters.append(clusters[-1])\n",
        "with open(\"/content/drive/MyDrive/annotation/hm_img_confounder.pickle\", \"wb\") as f:\n",
        "    pickle.dump(clusters, f)"
      ],
      "metadata": {
        "id": "75TxHerTPVWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(img_dup_all),len(clusters)"
      ],
      "metadata": {
        "id": "cjpSXOzzN3wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### label to dataframe"
      ],
      "metadata": {
        "id": "jIcRFseY5nSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/annotation/hm_img_confounder.pickle\", \"rb\") as f:\n",
        "    clusters = pickle.load(f)"
      ],
      "metadata": {
        "id": "6DcW_Uqz5tt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prep image clustered df\n",
        "df_img_cf = pd.DataFrame([img_dup_all, clusters]).T\n",
        "df_img_cf.columns = [\"img_id\", \"img_cluster\"]\n",
        "# remove single-image cluster\n",
        "df_img_cf_cnt = df_img_cf.groupby(\"img_cluster\").count().reset_index()\n",
        "single_cluster = df_img_cf_cnt.loc[df_img_cf_cnt[\"img_id\"]==1, \"img_cluster\"].tolist()\n",
        "print(len(df_img_cf))\n",
        "df_img_cf = df_img_cf[~df_img_cf[\"img_cluster\"].isin(single_cluster)]\n",
        "print(len(df_img_cf))\n",
        "# merge label\n",
        "df_img_cf = df_img_cf.merge(df[[\"id\", \"label\"]].drop_duplicates().set_index(\"id\"), left_on=\"img_id\", right_index=True)\n",
        "df_img_cf_cnt = df_img_cf.groupby(\"img_cluster\")[\"label\"].sum().reset_index()\n",
        "all_negatives = df_img_cf_cnt.loc[df_img_cf_cnt[\"label\"]==0, \"img_cluster\"].tolist()\n",
        "print(len(df_img_cf))\n",
        "df_img_cf = df_img_cf[~df_img_cf[\"img_cluster\"].isin(all_negatives)]\n",
        "print(len(df_img_cf))\n",
        "# add id by labels\n",
        "## id of positive image, youngest if multiple\n",
        "df_img_cf_pos = df_img_cf[df_img_cf[\"label\"]==1].groupby(\"img_cluster\")[\"img_id\"].min().reset_index()\n",
        "df_img_cf = df_img_cf.merge(df_img_cf_pos[[\"img_cluster\", \"img_id\"]].rename({\"img_id\": img_id_col}, axis=1).set_index(\"img_cluster\"), left_on=\"img_cluster\", right_index=True)\n",
        "df_img_cf.head()\n",
        "# df_img_cf_pos.head()\n",
        "# df_img_cf_cnt.head()"
      ],
      "metadata": {
        "id": "kx2usNFjPxH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### group for evaluation"
      ],
      "metadata": {
        "id": "U1HfMYQg523d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(df), len(df_img_cf), len(df_txt_cf)"
      ],
      "metadata": {
        "id": "1QJEpG486ToZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n",
        "print(df_img_cf.columns)\n",
        "print(df_txt_cf.columns)"
      ],
      "metadata": {
        "id": "MkAvs3SL61nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = df[[\"id\", \"split\", \"label\"]].merge(df_img_cf[[\"img_id\", \"img_org_id\"]].set_index(\"img_id\"),left_on=\"id\", right_index=True, how=\"left\").fillna(\"No Confounder\")\n",
        "df_merged = df_merged.merge(df_txt_cf[[\"id\", \"txt_org_id\"]].set_index(\"id\"),left_on=\"id\", right_index=True, how=\"left\").fillna(\"No Confounder\")"
      ],
      "metadata": {
        "id": "k6U-BIog-kF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.groupby(['split', 'label'])[\"img_org_id\"].nunique()"
      ],
      "metadata": {
        "id": "ANGTVAx8APe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.groupby(['split', 'label'])[\"txt_org_id\"].nunique()"
      ],
      "metadata": {
        "id": "xBoUogwFBZC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.to_parquet(\"/content/drive/MyDrive/annotation/confounders.parquet\", index=False)"
      ],
      "metadata": {
        "id": "ZBDInevPBszH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EOS"
      ],
      "metadata": {
        "id": "HEj3AfN37z2A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FtyJh1Ay70vU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}